{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invasive Species Monitoring\n",
    "\n",
    "This is my solution to the Kaggle Invasive Species Monitoring competition. The task was to identify images of invasive hydrangea.\n",
    "\n",
    "https://www.kaggle.com/c/invasive-species-monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.5.0\n",
      "Uninstalling tensorflow-2.5.0:\n",
      "  Successfully uninstalled tensorflow-2.5.0\n",
      "Found existing installation: Keras 2.4.3\n",
      "Uninstalling Keras-2.4.3:\n",
      "  Successfully uninstalled Keras-2.4.3\n",
      "Found existing installation: keras-nightly 2.5.0.dev2021032900\n",
      "Uninstalling keras-nightly-2.5.0.dev2021032900:\n",
      "  Successfully uninstalled keras-nightly-2.5.0.dev2021032900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping tf-nightly as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.6.0-cp38-cp38-win_amd64.whl (423.2 MB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (3.18.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: clang~=5.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\preet\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorflow) (0.14.0)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Using cached grpcio-1.40.0-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Collecting keras~=2.6\n",
      "  Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Installing collected packages: grpcio, tensorflow-estimator, keras, tensorflow\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "Successfully installed grpcio-1.40.0 keras-2.6.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow keras tf-nightly keras-nightly\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.6.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\preet\\anaconda3\\lib\\site-packages\n",
      "Requires: h5py, keras, opt-einsum, termcolor, grpcio, typing-extensions, clang, astunparse, numpy, tensorflow-estimator, tensorboard, google-pasta, wheel, wrapt, keras-preprocessing, protobuf, flatbuffers, absl-py, six, gast\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/train/1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/train/2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/train/3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/train/4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/train/5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  invasive\n",
       "0  ../input/train/1.jpg         0\n",
       "1  ../input/train/2.jpg         0\n",
       "2  ../input/train/3.jpg         1\n",
       "3  ../input/train/4.jpg         0\n",
       "4  ../input/train/5.jpg         1"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../input/train/\"\n",
    "\n",
    "def load_train(path):\n",
    "    train_set = pd.read_csv('../input/train_labels.csv')\n",
    "    train_label = np.array(train_set['invasive'].iloc[: ])\n",
    "    train_files = []\n",
    "    for i in range(len(train_set)):\n",
    "        train_files.append(path + str(int(train_set.iloc[i][0])) +'.jpg')\n",
    "    train_set['name'] = train_files\n",
    "    return train_files, train_set, train_label\n",
    "\n",
    "train_files, train_set, train_label = load_train(path)\n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1       0.5\n",
       "1     2       0.5\n",
       "2     3       0.5\n",
       "3     4       0.5\n",
       "4     5       0.5"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../input/test/\"\n",
    "\n",
    "def load_test(path):\n",
    "    test_set = pd.read_csv('../input/sample_submission.csv')\n",
    "    test_files = []\n",
    "    for i in range(len(test_set)):\n",
    "        test_files.append(path + str(int(test_set.iloc[i][0])) +'.jpg')\n",
    "    return test_files, test_set\n",
    "\n",
    "test_files, test_set = load_test(path)\n",
    "\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets\n",
    "Since we will be using a generator we don't need to actually load in any files into memory, all we need is the filepaths :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/train/1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/train/2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/train/3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/train/4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/train/5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  invasive\n",
       "0  ../input/train/1.jpg         0\n",
       "1  ../input/train/2.jpg         0\n",
       "2  ../input/train/3.jpg         1\n",
       "3  ../input/train/4.jpg         0\n",
       "4  ../input/train/5.jpg         1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files, train_set = util.load_train()\n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/test/1.jpg</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/test/2.jpg</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/test/3.jpg</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/test/4.jpg</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/test/5.jpg</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  invasive\n",
       "0  ../input/test/1.jpg       0.5\n",
       "1  ../input/test/2.jpg       0.5\n",
       "2  ../input/test/3.jpg       0.5\n",
       "3  ../input/test/4.jpg       0.5\n",
       "4  ../input/test/5.jpg       0.5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files= util.load_test()\n",
    "test_set = pd.read_csv('../input/sample_submission.csv')\n",
    "test_set['name'] = test_files\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_61 (InputLayer)        [(None, 800, 800, 3)]     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2162 (Ba (None, 800, 800, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Functional)    (None, 23, 23, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 21,804,845\n",
      "Trainable params: 21,770,407\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_height = 800\n",
    "img_width = 800\n",
    "img_channels = 3\n",
    "img_dim = (img_height, img_width, img_channels)\n",
    "\n",
    "def inceptionv3(img_dim=img_dim):\n",
    "    input_tensor = Input(shape=img_dim)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=img_dim)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    return model\n",
    "\n",
    "model = inceptionv3()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "Here we use 5-fold cross-validation to train the model. Submission file is saved with the average of all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, epochs, img_size, x, y, test, n_fold, kf):\n",
    "    roc_auc = metrics.roc_auc_score\n",
    "    preds_train = np.zeros(len(x), dtype = np.float)\n",
    "    preds_test = np.zeros(len(test), dtype = np.float)\n",
    "    train_scores = []; valid_scores = []\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train = x.iloc[train_index]; x_valid = x.iloc[test_index]\n",
    "        y_train = y[train_index]; y_valid = y[test_index]\n",
    "\n",
    "        def augment(src, choice):\n",
    "            if choice == 0:\n",
    "                # Rotate 90\n",
    "                src = np.rot90(src, 1)\n",
    "            if choice == 1:\n",
    "                # flip vertically\n",
    "                src = np.flipud(src)\n",
    "            if choice == 2:\n",
    "                # Rotate 180\n",
    "                src = np.rot90(src, 2)\n",
    "            if choice == 3:\n",
    "                # flip horizontally\n",
    "                src = np.fliplr(src)\n",
    "            if choice == 4:\n",
    "                # Rotate 90 counter-clockwise\n",
    "                src = np.rot90(src, 3)\n",
    "            if choice == 5:\n",
    "                # Rotate 180 and flip horizontally\n",
    "                src = np.rot90(src, 2)\n",
    "                src = np.fliplr(src)\n",
    "            return src\n",
    "\n",
    "        def train_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_train), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(x_train))\n",
    "                    train_batch = x_train[start:end]\n",
    "                    for filepath, tag in train_batch.values:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        img = augment(img, np.random.randint(6))\n",
    "                        x_batch.append(img)\n",
    "                        y_batch.append(tag)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def valid_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_valid), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(x_valid))\n",
    "                    valid_batch = x_valid[start:end]\n",
    "                    for filepath, tag in valid_batch.values:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        img = augment(img, np.random.randint(6))\n",
    "                        x_batch.append(img)\n",
    "                        y_batch.append(tag)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def test_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(test), batch_size):\n",
    "                    x_batch = []\n",
    "                    end = min(start + batch_size, len(test))\n",
    "                    test_batch = test[start:end]\n",
    "                    for filepath in test_batch:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        x_batch.append(img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    yield x_batch\n",
    "\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=1, \n",
    "                               verbose=1, min_lr=1e-7),\n",
    "             ModelCheckpoint(filepath='inception.fold_' + str(i) + '.hdf5', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')]\n",
    "\n",
    "        train_steps = len(x_train) / batch_size\n",
    "        valid_steps = len(x_valid) / batch_size\n",
    "        test_steps = len(test) / batch_size\n",
    "        \n",
    "        model = model\n",
    "\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', \n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "        model.fit_generator(train_generator(), train_steps, epochs=epochs, verbose=1, \n",
    "                            callbacks=callbacks, validation_data=valid_generator(), \n",
    "                            validation_steps=valid_steps)\n",
    "\n",
    "        model.load_weights(filepath='inception.fold_' + str(i) + '.hdf5')\n",
    "\n",
    "        print('Running validation predictions on fold {}'.format(i))\n",
    "        preds_valid = model.predict_generator(generator=valid_generator(),\n",
    "                                      steps=valid_steps, verbose=1)[:, 0]\n",
    "\n",
    "        print('Running train predictions on fold {}'.format(i))\n",
    "        preds_train = model.predict_generator(generator=train_generator(),\n",
    "                                      steps=train_steps, verbose=1)[:, 0]\n",
    "\n",
    "        valid_score = roc_auc(y_valid, preds_valid)\n",
    "        train_score = roc_auc(y_train, preds_train)\n",
    "        print('Val Score:{} for fold {}'.format(valid_score, i))\n",
    "        print('Train Score: {} for fold {}'.format(train_score, i))\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        print('Avg Train Score:{0:0.5f}, Val Score:{1:0.5f} after {2:0.5f} folds'.format\n",
    "              (np.mean(train_scores), np.mean(valid_scores), i))\n",
    "\n",
    "        print('Running test predictions with fold {}'.format(i))\n",
    "\n",
    "        preds_test_fold = model.predict_generator(generator=test_generator(),\n",
    "                                              steps=test_steps, verbose=1)[:, -1]\n",
    "\n",
    "        preds_test += preds_test_fold\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i <= n_fold:\n",
    "            print('Now beginning training for fold {}\\n\\n'.format(i))\n",
    "        else:\n",
    "            print('Finished training!')\n",
    "\n",
    "    preds_test /= n_fold\n",
    "\n",
    "\n",
    "    return preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Preet\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Preet\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1935: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  Raises:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002849E8103A0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002849E8103A0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "202/367 [===============>..............] - ETA: 42:16 - loss: 0.2772 - accuracy: 0.8911"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-afb0b72a15bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m test_pred = train_model(model, batch_size, epochs, img_size, train_set, \n\u001b[0m\u001b[0;32m      8\u001b[0m                         train_label, test_files, n_fold, kf)\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-cde58208ef5a>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, batch_size, epochs, img_size, x, y, test, n_fold, kf)\u001b[0m\n\u001b[0;32m     95\u001b[0m                       metrics = ['accuracy'])\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         model.fit_generator(train_generator(), train_steps, epochs=epochs, verbose=1, \n\u001b[0m\u001b[0;32m     98\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                             validation_steps=valid_steps)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mwrapped\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1937\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmismatch\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1938\u001b[1;33m           \u001b[0mexpectations\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1939\u001b[0m     \"\"\"\n\u001b[0;32m   1940\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict_on_batch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1176\u001b[0m       \u001b[1;31m# TODO(omalleyt): Fix the ordering issues that mean this has to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m       \u001b[1;31m# happen after `callbacks.on_train_begin`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m       data_handler._initial_epoch = (  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   1179\u001b[0m           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[0;32m   1180\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[0mwithout_tracing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnew_tracing_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m       \u001b[0mexecution_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"notTraced\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwithout_tracing\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"traced\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m       tm.set_metadata(tf_function_call=execution_mode + \"-\" + compiler,\n\u001b[0;32m    891\u001b[0m                       tracing_count=new_tracing_count)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3023\u001b[1;33m       self._hashable_input_signature = _make_input_signature_hashable(\n\u001b[0m\u001b[0;32m   3024\u001b[0m           self.flat_input_signature)\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_inputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m     \u001b[0mpossible_gradient_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradients_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPossibleTapeGradientTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m     if (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0m\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "epochs = 50\n",
    "n_fold = 5\n",
    "img_size = (img_height, img_width)\n",
    "kf = KFold(n_splits=n_fold, shuffle=True)\n",
    "\n",
    "test_pred = train_model(model, batch_size, epochs, img_size, train_set, \n",
    "                        train_label, test_files, n_fold, kf)\n",
    "\n",
    "test_set['invasive'] = test_pred\n",
    "test_set.to_csv('./submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Psuedo Labeling\n",
    "Once you have a good scoring model you can use its predictions to expand your training set, this is called pseudo labeling and is a semi-supervised learning technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use submission from best scoring model to create semi-supervised dataset\n",
    "path = './submissions/submission.csv'\n",
    "    \n",
    "pred_set = util.get_pred_set(path, test_files)\n",
    "\n",
    "# Only use subset of semi-supervised dataset\n",
    "pred_set = pred_set[:1000]\n",
    "\n",
    "# Combine training set with semi-supervised training set\n",
    "pseudo_set = pd.concat([train_set, pred_set], axis=0)\n",
    "\n",
    "pseudo_label = np.array(pseudo_set['invasive'].iloc[:])\n",
    "\n",
    "print('We combined {} training images with {} test predicted images and now have a combined training set of {} images'.format\n",
    "      (len(train_set), len(pred_set), len(pseudo_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model w/ Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = util.train_model(model, batch_size, epochs, img_size, pseudo_set, pseudo_label, test_files, n_fold, kf)\n",
    "\n",
    "test_set['invasive'] = test_pred\n",
    "test_set.to_csv('./submissions/submission.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
